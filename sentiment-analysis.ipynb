{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§  Step 1: Install Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q \"pyarrow<20.0.0\" \"rich<14\" \"google-cloud-bigquery-storage>=2.30.0,<3.0.0\" \\\n    transformers==4.44.0 datasets==3.0.0 accelerate==0.33.0 evaluate==0.4.2 scikit-learn==1.5.2\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:35:45.827006Z","iopub.execute_input":"2025-11-07T20:35:45.827263Z","iopub.status.idle":"2025-11-07T20:37:23.291132Z","shell.execute_reply.started":"2025-11-07T20:35:45.827245Z","shell.execute_reply":"2025-11-07T20:37:23.290363Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.6.1 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers, datasets, torch\nprint(transformers.__version__)\nprint(\"âœ… Environment ready for Hugging Face on Kaggle!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:38:04.468785Z","iopub.execute_input":"2025-11-07T20:38:04.469542Z","iopub.status.idle":"2025-11-07T20:38:09.087172Z","shell.execute_reply.started":"2025-11-07T20:38:04.469505Z","shell.execute_reply":"2025-11-07T20:38:09.086312Z"}},"outputs":[{"name":"stdout","text":"4.44.0\nâœ… Environment ready for Hugging Face on Kaggle!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# ðŸ§¾ Step 2: Import Dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport evaluate\n\n# âœ… Confirm GPU availability in Kaggle\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"ðŸ”¥ Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:38:54.076459Z","iopub.execute_input":"2025-11-07T20:38:54.077273Z","iopub.status.idle":"2025-11-07T20:39:16.167370Z","shell.execute_reply.started":"2025-11-07T20:38:54.077245Z","shell.execute_reply":"2025-11-07T20:39:16.166628Z"}},"outputs":[{"name":"stderr","text":"2025-11-07 20:38:57.488594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762547937.748201      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762547937.814038      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ðŸ”¥ Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# ðŸ“¥ Step 3 â€” Load and Inspect Dataset","metadata":{}},{"cell_type":"code","source":"# Load the IMDB sentiment dataset (binary classification: positive/negative)\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"imdb\")\n\n# Check basic structure\nprint(dataset)\nprint(dataset[\"train\"][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:40:13.242319Z","iopub.execute_input":"2025-11-07T20:40:13.243475Z","iopub.status.idle":"2025-11-07T20:40:17.867521Z","shell.execute_reply.started":"2025-11-07T20:40:13.243449Z","shell.execute_reply":"2025-11-07T20:40:17.866738Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75202c77c00b4340a92c3111a6a26bd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d8b4172455c4e8bb2a02e2955ad9874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a6579cd29c4306b78e56f6077a54c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/unsupervised-00000-of-00001.p(â€¦):   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a683ab51a0d48daa96c7089bd764cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eeaea43bd4046f8ad2e5a6289946a65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80ba467064684d628155be280d8bbae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ebb8304380744b4ae98115085698056"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# ðŸ”¤ Step 4 â€” Tokenize the Text\n\n## Weâ€™ll use a small, fast model to keep Kaggle GPU within limits.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Tokenisation function\ndef tokenize_function(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\n# Apply tokenization to dataset\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\ntokenized_datasets.set_format(\"torch\")\n\n# For quick run, take subset (optional)\nsmall_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(4000))\nsmall_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2000))\n\nprint(small_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:40:49.314222Z","iopub.execute_input":"2025-11-07T20:40:49.314879Z","iopub.status.idle":"2025-11-07T20:41:36.534566Z","shell.execute_reply.started":"2025-11-07T20:40:49.314854Z","shell.execute_reply":"2025-11-07T20:41:36.533965Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f078aa17689c4e92855539aef920e243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d18a344d51d45a7ac3e5ab18cf1b32e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eb3b79f9927484da12cf8c64a52f7d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d287bcdf81be4dac91a787eb9891f8f8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c196ddced614bb09856ba9edcfc359c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8785a099d3498eba4c186b931e1f0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d374da092f6f49e99596a492cc164fe2"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['label', 'input_ids', 'attention_mask'],\n    num_rows: 4000\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# ðŸ§  Step 5 â€” Load Model aur Define Evaluation Metrics\n\n## Run cell ðŸ‘‡","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# Pre-trained DistilBERT model for sentiment classification (2 classes)\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\n# Metric calculation function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:41:56.989670Z","iopub.execute_input":"2025-11-07T20:41:56.989999Z","iopub.status.idle":"2025-11-07T20:41:58.433643Z","shell.execute_reply.started":"2025-11-07T20:41:56.989976Z","shell.execute_reply":"2025-11-07T20:41:58.433080Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b640719b7ee491f9d775196d117c94b"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# âš™ï¸ Step 6 â€” Define Training Arguments (Kaggle-Optimized)\n\n## GPU safe configuration ðŸ‘‡","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/sentiment_results\",\n    run_name=\"sentiment_run_v1\",  # âœ… unique name to silence WandB warning\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir=\"/kaggle/working/logs\",\n    logging_steps=100,\n    report_to=\"none\",  # âœ… fully disable wandb & tensorboard logs\n    fp16=torch.cuda.is_available(),  # GPU mixed precision\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:45:28.704722Z","iopub.execute_input":"2025-11-07T20:45:28.705350Z","iopub.status.idle":"2025-11-07T20:45:28.741485Z","shell.execute_reply.started":"2025-11-07T20:45:28.705322Z","shell.execute_reply":"2025-11-07T20:45:28.740747Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# ðŸ§© Step 7 â€” Trainer Object","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train,\n    eval_dataset=small_test,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:45:30.758812Z","iopub.execute_input":"2025-11-07T20:45:30.759476Z","iopub.status.idle":"2025-11-07T20:45:30.772811Z","shell.execute_reply.started":"2025-11-07T20:45:30.759448Z","shell.execute_reply":"2025-11-07T20:45:30.771852Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# ðŸ‹ï¸ Step 8 â€” Train the Model\n\n## Run cell ðŸ‘‡","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:45:33.223129Z","iopub.execute_input":"2025-11-07T20:45:33.223398Z","iopub.status.idle":"2025-11-07T20:48:15.351647Z","shell.execute_reply.started":"2025-11-07T20:45:33.223380Z","shell.execute_reply":"2025-11-07T20:48:15.350803Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.352000</td>\n      <td>0.277020</td>\n      <td>0.883000</td>\n      <td>0.902311</td>\n      <td>0.859000</td>\n      <td>0.880123</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.194400</td>\n      <td>0.283006</td>\n      <td>0.892000</td>\n      <td>0.885069</td>\n      <td>0.901000</td>\n      <td>0.892963</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.30547201919555667, metrics={'train_runtime': 161.6648, 'train_samples_per_second': 49.485, 'train_steps_per_second': 3.093, 'total_flos': 529869594624000.0, 'train_loss': 0.30547201919555667, 'epoch': 2.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# ðŸ“Š Step 9 â€” Evaluate Trained Model","metadata":{}},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(\"ðŸ“ˆ Evaluation Results:\")\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:49:26.658114Z","iopub.execute_input":"2025-11-07T20:49:26.658453Z","iopub.status.idle":"2025-11-07T20:49:38.071784Z","shell.execute_reply.started":"2025-11-07T20:49:26.658430Z","shell.execute_reply":"2025-11-07T20:49:38.071099Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"ðŸ“ˆ Evaluation Results:\n{'eval_loss': 0.28300580382347107, 'eval_accuracy': 0.892, 'eval_precision': 0.8850687622789783, 'eval_recall': 0.901, 'eval_f1': 0.8929633300297324, 'eval_runtime': 11.4049, 'eval_samples_per_second': 175.364, 'eval_steps_per_second': 10.96, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# ðŸ’¬ Step 10 â€” Custom Text Sentiment Prediction\n\n## text model predict ðŸ‘‡","metadata":{}},{"cell_type":"code","source":"# Custom text examples\ntexts = [\n    \"I absolutely loved this movie, it was emotional and inspiring!\",\n    \"The plot was weak and the acting was horrible.\",\n    \"It was an average experience, not great but not bad either.\"\n]\n\ntokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\noutputs = model(**tokens)\npredictions = outputs.logits.argmax(dim=1)\n\nfor text, pred in zip(texts, predictions):\n    sentiment = \"Positive ðŸ˜Š\" if pred == 1 else \"Negative ðŸ˜ž\"\n    print(f\"{text}\\nâ†’ {sentiment}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:50:40.198995Z","iopub.execute_input":"2025-11-07T20:50:40.199604Z","iopub.status.idle":"2025-11-07T20:50:40.236272Z","shell.execute_reply.started":"2025-11-07T20:50:40.199579Z","shell.execute_reply":"2025-11-07T20:50:40.235601Z"}},"outputs":[{"name":"stdout","text":"I absolutely loved this movie, it was emotional and inspiring!\nâ†’ Positive ðŸ˜Š\n\nThe plot was weak and the acting was horrible.\nâ†’ Negative ðŸ˜ž\n\nIt was an average experience, not great but not bad either.\nâ†’ Positive ðŸ˜Š\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# ðŸ’¾ (Optional) Step 11 â€” Save Model for Kaggle Output Folder","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/sentiment_model\")\ntokenizer.save_pretrained(\"/kaggle/working/sentiment_model\")\nprint(\"âœ… Model saved to /kaggle/working/sentiment_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:52:28.131309Z","iopub.execute_input":"2025-11-07T20:52:28.131923Z","iopub.status.idle":"2025-11-07T20:52:28.706170Z","shell.execute_reply.started":"2025-11-07T20:52:28.131889Z","shell.execute_reply":"2025-11-07T20:52:28.705312Z"}},"outputs":[{"name":"stdout","text":"âœ… Model saved to /kaggle/working/sentiment_model\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}